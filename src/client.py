import warnings  # ç”¨æ–¼å¿½ç•¥è­¦å‘Šè¨Šæ¯
from collections import OrderedDict  # ç”¨æ–¼æœ‰åºå­—å…¸ï¼Œå¹«åŠ©æ¨¡å‹åƒæ•¸ç®¡ç†
import flwr as fl  # Flower è¯é‚¦å­¸ç¿’æ¡†æ¶
import torch  # PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶
import torch.nn as nn  # PyTorch ç¥ç¶“ç¶²çµ¡æ¨¡çµ„
# import torch.nn.functional as F  # PyTorch å‡½æ•¸æ¨¡çµ„ï¼ˆå¦‚æ¿€æ´»å‡½æ•¸ï¼‰
from torch.utils.data import DataLoader, TensorDataset  # PyTorch è³‡æ–™è¼‰å…¥å™¨èˆ‡è³‡æ–™é›†
from tqdm import tqdm  # é€²åº¦æ¢é¡¯ç¤ºå·¥å…·
# import argparse  # å‘½ä»¤åˆ—åƒæ•¸è§£æå™¨
import pandas as pd  # è³‡æ–™è™•ç†åº«ï¼Œç”¨æ–¼è®€å– CSV ç­‰
import os  # ä½œæ¥­ç³»çµ±ç›¸é—œæ“ä½œï¼Œå¦‚è·¯å¾‘è™•ç†
import numpy as np  # æ•¸å€¼è¨ˆç®—åº«
from sklearn.pipeline import Pipeline  # sklearn ç®¡é“ï¼Œç”¨æ–¼è³‡æ–™é è™•ç†æµç¨‹
from sklearn.impute import SimpleImputer  # ç¼ºå¤±å€¼å¡«è£œå™¨
from sklearn.preprocessing import ( StandardScaler, MinMaxScaler,
     OneHotEncoder, OrdinalEncoder, LabelEncoder)  # è³‡æ–™é è™•ç†å·¥å…·ï¼ˆå¦‚ç¸®æ”¾ã€ç·¨ç¢¼ï¼‰
from sklearn.compose import ColumnTransformer  # æ¬„ä½è½‰æ›å™¨
from sklearn.model_selection import train_test_split  # è³‡æ–™åˆ‡åˆ†å·¥å…·
from sklearn.metrics import f1_score  # F1 åˆ†æ•¸è©•ä¼°æŒ‡æ¨™
from sklearn.utils.class_weight import compute_class_weight  # è¨ˆç®—é¡åˆ¥æ¬Šé‡ï¼Œç”¨æ–¼ä¸å¹³è¡¡è³‡æ–™
from flwr.app import Context
from flwr.clientapp import ClientApp
import json
import hashlib
from pathlib import Path
import os, time

#cache è·¯å¾‘å·¥å…·
def _get_cache_paths(datapath: str) -> tuple[Path, Path, Path]:
    cache_root = Path(os.environ.get("CACHE_DIR", "/app/cache"))
    pod = os.environ.get("POD_NAME", "pod-unknown")
    cache_dir = cache_root / pod
    cache_dir.mkdir(parents=True, exist_ok=True)

    npz_path = cache_dir / "dataset.npz"
    meta_path = cache_dir / "meta.json"
    lock_path = cache_dir / ".lock"
    return npz_path, meta_path, lock_path

def _file_sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def _current_cache_meta(csv_path: str) -> dict:
    return {
        "csv_sha256": _file_sha256(csv_path),
        "scaling": scaling,
        "encoding": encoding,
        "drop_empty_rows": drop_empty_rows,
        "split": {"test_size": 0.2, "val_size": 0.2, "random_state": 42},
        "version": 1,
    }

def _meta_matches(meta_path: Path, current: dict) -> bool:
    try:
        if not meta_path.exists():
            return False
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
        return meta == current
    except Exception:
        return False

# ========= cache (per-pod / per-process) =========
_DATA_CACHE = None          # tuple
_DATA_CACHE_PATH = None     # str

# 1. Regular PyTorch pipeline: nn.Module, train, test, and DataLoader

warnings.filterwarnings("ignore", category=UserWarning)  # å¿½ç•¥ä½¿ç”¨è€…è­¦å‘Š
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # è¨­å®šè£ç½®ç‚º GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰å¦å‰‡ CPU

# æ¨¡å‹å®šç¾©ï¼šæ·ºå±¤ MLPï¼Œç”¨æ–¼ IDS è¡¨æ ¼ç‰¹å¾µ
class Net(nn.Module):
    """Shallow MLP for IDS tabular features"""

    def __init__(self, input_dim=110, hidden=[8, 8], n_classes=1):
        super(Net, self).__init__()  # å‘¼å«çˆ¶é¡åˆå§‹åŒ–
        layers = []  # å±¤åˆ—è¡¨
        prev = input_dim  # å‰ä¸€å±¤è¼¸å‡ºç¶­åº¦åˆå§‹åŒ–ç‚ºè¼¸å…¥ç¶­åº¦

        # fc0 + fc1ï¼šéš±è—å±¤
        for h in hidden:  # ä¾éš±è—å±¤å¤§å°è¿´åœˆ
            layers.append(nn.Linear(prev, h))  # æ·»åŠ å…¨é€£æ¥å±¤
            layers.append(nn.ReLU())  # æ·»åŠ  ReLU æ¿€æ´»
            prev = h  # æ›´æ–°å‰ä¸€å±¤è¼¸å‡ºç¶­åº¦

        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev, n_classes))  # æ·»åŠ è¼¸å‡ºå…¨é€£æ¥å±¤
        self.net = nn.Sequential(*layers)  # å°‡å±¤åˆ—è¡¨è½‰ç‚º Sequential æ¨¡å‹

        # æ ¹æ“šä»»å‹™æ±ºå®šè¼¸å‡ºæ¿€æ´»
        self.is_binary = n_classes == 1  # åˆ¤æ–·æ˜¯å¦ç‚ºäºŒå…ƒåˆ†é¡

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # å‰å‘å‚³æ’­
        return self.net(x)  # é€šéæ¨¡å‹è¨ˆç®—è¼¸å‡º

# è¨“ç·´å‡½æ•¸ï¼šè¨“ç·´æ¨¡å‹ä¸¦åœ¨æ¯å€‹ epoch å¾Œé©—è­‰
def train(net, trainloader, valloader, epochs, class_weights, is_binary):
    """Train the model on the training set with validation."""
    if is_binary:  # äºŒå…ƒåˆ†é¡
        pos_weight = (
            torch.tensor([class_weights[0] / class_weights[1]]).to(DEVICE)
            if class_weights is not None 
            else None)  # è¨ˆç®—æ­£é¡æ¬Šé‡
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # BCE æå¤±ï¼ˆå¸¶ logitsï¼‰
    else:  # å¤šé¡åˆ†é¡
        criterion = nn.CrossEntropyLoss(
            weight=class_weights.to(DEVICE) 
            if class_weights is not None 
            else None)  # äº¤å‰ç†µæå¤±
    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)  # Adam æœ€ä½³åŒ–å™¨
    for epoch in range(epochs):  # æ¯å€‹ epoch è¿´åœˆ
        net.train()  # è¨­å®šæ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
        for inputs, labels in tqdm(trainloader):  # éæ­·è¨“ç·´è³‡æ–™
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)  # ç§»åˆ°è£ç½®
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            outputs = net(inputs)  # å‰å‘è¨ˆç®—
            if is_binary:  # äºŒå…ƒæå¤±è¨ˆç®—
                loss = criterion(outputs, labels.float())
            else:  # å¤šé¡æå¤±è¨ˆç®—
                loss = criterion(outputs, labels)
            loss.backward()  # åå‘å‚³æ’­
            optimizer.step()  # æ›´æ–°åƒæ•¸
        # æ¯å€‹ epoch å¾Œé©—è­‰
        val_loss, val_acc, val_f1 = test(net, valloader, is_binary)  # è¨ˆç®—é©—è­‰æŒ‡æ¨™
        print(
            f"Epoch {epoch+1}/{epochs}"
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\
            , Val F1: {val_f1:.4f}")  # å°å‡ºé©—è­‰çµæœ

# æ¸¬è©¦å‡½æ•¸ï¼šè©•ä¼°æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾
def test(net, testloader, is_binary):
    """Validate the model on the test set."""
    if is_binary:  # äºŒå…ƒåˆ†é¡æå¤±
        criterion = nn.BCEWithLogitsLoss()
    else:  # å¤šé¡åˆ†é¡æå¤±
        criterion = nn.CrossEntropyLoss()
    correct, total, loss = 0, 0, 0.0  # åˆå§‹åŒ–æ­£ç¢ºæ•¸ã€ç¸½æ•¸ã€æå¤±
    all_preds, all_labels = [], []  # æ”¶é›†æ‰€æœ‰é æ¸¬èˆ‡æ¨™ç±¤
    with torch.no_grad():  # ç„¡æ¢¯åº¦æ¨¡å¼
        for inputs, labels in tqdm(testloader):  # éæ­·æ¸¬è©¦è³‡æ–™
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)  # ç§»åˆ°è£ç½®
            outputs = net(inputs)  # å‰å‘è¨ˆç®—
            if is_binary:  # äºŒå…ƒè™•ç†
                batch_loss = criterion(outputs, labels.float()).item()  # æ‰¹æ¬¡æå¤±
                pred = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()  # é æ¸¬ï¼ˆ>0.5ï¼‰
                labels_np = labels.cpu().numpy()  # æ¨™ç±¤è½‰ numpy
            else:  # å¤šé¡è™•ç†
                batch_loss = criterion(outputs, labels).item()  # æ‰¹æ¬¡æå¤±
                pred = torch.argmax(outputs, dim=1).cpu().numpy()  # argmax é æ¸¬
                labels_np = labels.cpu().numpy()  # æ¨™ç±¤è½‰ numpy
            loss += batch_loss * labels.size(0)  # ç´¯åŠ æå¤±
            total += labels.size(0)  # ç´¯åŠ ç¸½æ•¸
            correct += (pred == labels_np).sum()  # ç´¯åŠ æ­£ç¢ºæ•¸
            all_preds.extend(pred)  # æ”¶é›†é æ¸¬
            all_labels.extend(labels_np)  # æ”¶é›†æ¨™ç±¤
    avg_loss = loss / total  # å¹³å‡æå¤±
    acc = correct / total  # æº–ç¢ºç‡
    if is_binary:  # äºŒå…ƒ F1
        f1 = f1_score(all_labels, all_preds, average='binary')
    else:  # å¤šé¡ F1 (macro)
        f1 = f1_score(all_labels, all_preds, average='macro')
    return avg_loss, acc, f1  # è¿”å›æå¤±ã€æº–ç¢ºç‡ã€F1

# 1 å·¥å…·ï¼šæ¬„åæ¸…ç†/æ¨™ç±¤åµæ¸¬
def clean_col(c: str) -> str:  # æ¸…ç†æ¬„ä½åç¨±
    c = str(c).strip()  # ç§»é™¤å‰å¾Œç©ºæ ¼
    c = "".join(ch for ch in c if ch.isprintable())  # ä¿ç•™å¯åˆ—å°å­—å…ƒ
    c = c.replace("\u200b", "").replace("\ufeff", "")  # ç§»é™¤ç‰¹å®š Unicode
    c = "_".join(c.split())  # ç©ºæ ¼è½‰ä¸‹åŠƒç·š
    return c

def auto_pick_label(df: pd.DataFrame) -> str:  # è‡ªå‹•é¸æ“‡æ¨™ç±¤æ¬„
    candidates = ["label","Label","target","Target","class","Class",
                  "attack_cat","Attack","is_attack","y"]  # å€™é¸æ¬„å
    for c in candidates:  # æª¢æŸ¥æ˜¯å¦å­˜åœ¨
        if c in df.columns:
            return c
    raise ValueError("æ‰¾ä¸åˆ°æ¨™ç±¤æ¬„ï¼Œè«‹å°‡è®Šæ•¸ label_col è¨­ç‚ºä½ çš„æ¨™ç±¤æ¬„åã€‚")

# 2 å‰è™•ç†ï¼ˆfit åœ¨è¨“ç·´é›†ï¼‰
def fit_preprocessor(X_train: pd.DataFrame):  # æ“¬åˆé è™•ç†å™¨
    # é¡åˆ¥æ¬„ï¼šobject æˆ– categoryï¼›å…¶é¤˜è¦–ç‚ºæ•¸å€¼
    cat_cols = [
        c
        for c in X_train.columns
        if (X_train[c].dtype == "object" or str(X_train[c].dtype).startswith("category"))]  # é¡åˆ¥æ¬„åˆ—è¡¨
    num_cols = [c for c in X_train.columns if c not in cat_cols]  # æ•¸å€¼æ¬„åˆ—è¡¨

    # æ•¸å€¼ï¼šä¸­ä½æ•¸ â†’ ç¸®æ”¾
    num_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),  # ä¸­ä½æ•¸å¡«è£œ
            ("scaler", StandardScaler() if scaling == "standard" else MinMaxScaler())  # æ¨™æº–åŒ–æˆ– MinMax ç¸®æ”¾
        ]
    )

    # é¡åˆ¥ï¼šçœ¾æ•¸ â†’ ç·¨ç¢¼
    if encoding == "onehot":  # OneHot ç·¨ç¢¼
        cat_encoder = OneHotEncoder(
            handle_unknown="ignore", sparse_output=False)
    else:  # Ordinal ç·¨ç¢¼
        cat_encoder = OrdinalEncoder(
            handle_unknown="use_encoded_value", unknown_value=-1)

    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),  # çœ¾æ•¸å¡«è£œ
        ("encoder", cat_encoder)  # ç·¨ç¢¼å™¨
        ]
    )

    pre = ColumnTransformer(  # æ¬„ä½è½‰æ›å™¨
        transformers=[
            ("num", num_pipe, num_cols),  # æ•¸å€¼ç®¡é“
            ("cat", cat_pipe, cat_cols),  # é¡åˆ¥ç®¡é“
        ],
        remainder="drop"  # å¿½ç•¥å…¶é¤˜
    )
    pre.fit(X_train)  # æ“¬åˆè¨“ç·´é›†
    return pre, num_cols, cat_cols  # è¿”å›é è™•ç†å™¨èˆ‡æ¬„åˆ—è¡¨

def transform_with_preprocessor(pre, X_df: pd.DataFrame) -> np.ndarray:  # ä½¿ç”¨é è™•ç†å™¨è½‰æ›è³‡æ–™
    return pre.transform(X_df)  # è½‰æ›ä¸¦è¿”å› numpy é™£åˆ—

# 3 è³‡æ–™è®€å–èˆ‡åˆ‡åˆ†
def load_and_split(csv_path: str, label_col: str = None):  # è®€å– CSV ä¸¦åˆ‡åˆ†è³‡æ–™
    df = pd.read_csv(csv_path, low_memory=False)  # è®€å– CSV
    df.columns = [clean_col(c) for c in df.columns]  # æ¸…ç†æ¬„å
    if drop_empty_rows:  # è‹¥è¨­å®šï¼Œç§»é™¤å…¨ç©ºè¡Œ
        df = df.dropna(how="all")

    if label_col is None:  # è‡ªå‹•é¸æ“‡æ¨™ç±¤
        label = auto_pick_label(df)
    else:
        label = label_col
        if label not in df.columns:  # æª¢æŸ¥æ˜¯å¦å­˜åœ¨
            raise ValueError(f"æŒ‡å®šçš„æ¨™ç±¤æ¬„ `{label}` ä¸åœ¨ CSV æ¬„ä½ä¸­ã€‚")

    # è¦ç¯©é¸çš„ä¸‰ç¨®é¡åˆ¥ï¼ˆå°å¯«æ¯”å°ï¼‰
    TARGETS = {"dictionarybruteforce", "sqlinjection", "benigntraffic"}  # å®šç¾©ç›®æ¨™é¡åˆ¥ï¼ˆå°å¯«ï¼‰
    df[label] = df[label].str.lower().str.replace(" ", "")  # å°‡æ¨™ç±¤è½‰å°å¯«ä¸¦ç§»é™¤ç©ºæ ¼ï¼Œä»¥ç¢ºä¿æ¯”å°ä¸€è‡´

    # éæ¿¾åªä¿ç•™æŒ‡å®šçš„é¡åˆ¥
    df = df[df[label].isin(TARGETS)]  # éæ¿¾è³‡æ–™æ¡†ï¼Œåªä¿ç•™ç›®æ¨™é¡åˆ¥

    y_raw = df[label]  # åŸå§‹æ¨™ç±¤
    X_raw = df.drop(columns=[label])  # ç‰¹å¾µ

    # å…ˆåˆ‡ train/testï¼Œé¿å…è³‡æ–™å¤–æ´©
    y_for_split = y_raw.astype(str) if (y_raw.dtype == object or str(y_raw.dtype).startswith("category")) else y_raw  # ç”¨æ–¼åˆ†å±¤çš„æ¨™ç±¤
    X_train_full, X_test, y_train_full, y_test_raw = train_test_split(
        X_raw, y_for_split, test_size=0.2, random_state=42, stratify=y_for_split  # åˆ‡åˆ† 80/20
    )
    # å¾ train_full å†åˆ‡ val (20% of train_full)
    X_train, X_val, y_train_raw, y_val_raw = train_test_split(
        X_train_full, y_train_full, test_size=0.2, random_state=42,
        stratify=y_train_full  # å¾è¨“ç·´é›†åˆ‡ 80/20 ç‚º train/val
    )
    return X_train, X_val, X_test, y_train_raw, y_val_raw, y_test_raw, label  # è¿”å›åˆ‡åˆ†çµæœ

# 4 æ¨™ç±¤ç·¨ç¢¼ï¼ˆäºŒå…ƒ / å¤šé¡ï¼‰
def encode_labels(y_train_raw, y_val_raw, y_test_raw):  # ç·¨ç¢¼æ¨™ç±¤
    le = LabelEncoder()  # æ¨™ç±¤ç·¨ç¢¼å™¨
    y_train_idx = le.fit_transform(np.asarray(y_train_raw).astype(str))  # æ“¬åˆä¸¦è½‰æ›è¨“ç·´æ¨™ç±¤
    y_val_idx = le.transform(np.asarray(y_val_raw).astype(str))  # è½‰æ›é©—è­‰æ¨™ç±¤
    y_test_idx = le.transform(np.asarray(y_test_raw).astype(str))  # è½‰æ›æ¸¬è©¦æ¨™ç±¤
    classes = le.classes_  # é¡åˆ¥åˆ—è¡¨
    num_classes = len(classes)  # é¡åˆ¥æ•¸
    is_binary = (num_classes == 2)  # æ˜¯å¦äºŒå…ƒ
    return y_train_idx, y_val_idx, y_test_idx, num_classes, is_binary, le  # è¿”å›ç·¨ç¢¼çµæœ


class DataCache:
    """
    æŠŠ load_data() çš„ã€Œå‰è™•ç†è¼¸å‡ºã€è½ç›¤åˆ° cache_dir
    - X_train.npy, X_val.npy, X_test.npy
    - y_train.npy, y_val.npy, y_test.npy
    - class_weights.npy
    - meta.json
    """

    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # å›ºå®šæª”åï¼ˆæ–¹æ¡ˆ Aï¼‰
        self.x_train = self.cache_dir / "X_train.npy"
        self.x_val   = self.cache_dir / "X_val.npy"
        self.x_test  = self.cache_dir / "X_test.npy"
        self.y_train = self.cache_dir / "y_train.npy"
        self.y_val   = self.cache_dir / "y_val.npy"
        self.y_test  = self.cache_dir / "y_test.npy"
        self.cw      = self.cache_dir / "class_weights.npy"
        self.meta    = self.cache_dir / "meta.json"

        # å¯«å…¥é–ï¼ˆé¿å… 2 å€‹ client åŒæ™‚å»º cacheï¼‰
        self.lock = self.cache_dir / ".build.lock"

        # å‰è™•ç†ç‰ˆæœ¬è™Ÿ
        self.preprocessing_version = "v1"

    def _hash_file(self, path: Path, algo="sha256", chunk_size=1024 * 1024) -> str:
        h = hashlib.new(algo)
        with open(path, "rb") as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                h.update(chunk)
        return h.hexdigest()

    def _make_meta(self, csv_path: str, config: dict, extra: dict) -> dict:
        p = Path(csv_path)
        meta = {
            "preprocessing_version": self.preprocessing_version,
            "csv_path": str(p),
            "csv_hash": self._hash_file(p) if p.exists() else None,
            "csv_stat": {
                "size": p.stat().st_size if p.exists() else None,
                "mtime": p.stat().st_mtime if p.exists() else None,
            },
            "config": config,   # å‰è™•ç†çš„é—œéµè¨­å®šï¼ˆæœƒæ‹¿ä¾†æ¯”å°ï¼‰
            "extra": extra,     # ä¾‹å¦‚ï¼šclassesã€input_dimã€n_classesã€is_binary
        }
        return meta

    def _read_meta(self) -> dict:
        with open(self.meta, "r", encoding="utf-8") as f:
            return json.load(f)

    def _write_meta_atomic(self, meta: dict):
        tmp = self.cache_dir / "meta.json.tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        os.replace(tmp, self.meta)

    def _all_cache_files_exist(self) -> bool:
        return all([
            self.x_train.exists(), self.x_val.exists(), self.x_test.exists(),
            self.y_train.exists(), self.y_val.exists(), self.y_test.exists(),
            self.cw.exists(),
            self.meta.exists(),
        ])

    def is_valid(self, csv_path: str, config: dict) -> (bool, str):
        # æª¢æŸ¥ cache æ˜¯å¦å­˜åœ¨ä¸” meta åŒ¹é…
        if not self._all_cache_files_exist():
            return False, "cache files missing"

        try:
            meta = self._read_meta()
        except Exception:
            return False, "meta.json unreadable"

        if meta.get("preprocessing_version") != self.preprocessing_version:
            return False, "preprocessing_version mismatch"

        # æ¯”å° config
        if meta.get("config") != config:
            return False, "config mismatch"

        # æ¯”å° CSV hashï¼ˆç¢ºä¿åŸå§‹è³‡æ–™æ²’è®Šï¼‰
        p = Path(csv_path)
        if not p.exists():
            return False, "csv missing"

        current_hash = self._hash_file(p)
        if meta.get("csv_hash") != current_hash:
            return False, "csv hash mismatch"

        return True, "cache hit"

    def load(self):
        # å¾ cache è®€å› numpy arraysï¼ˆå†ç”±å¤–å±¤è½‰ torchï¼‰
        X_train = np.load(self.x_train, allow_pickle=False)
        X_val   = np.load(self.x_val, allow_pickle=False)
        X_test  = np.load(self.x_test, allow_pickle=False)
        y_train = np.load(self.y_train, allow_pickle=False)
        y_val   = np.load(self.y_val, allow_pickle=False)
        y_test  = np.load(self.y_test, allow_pickle=False)
        class_weights = np.load(self.cw, allow_pickle=False)

        meta = self._read_meta()
        extra = meta.get("extra", {})

        return X_train, X_val, X_test, y_train, y_val, y_test, class_weights, extra

    def _acquire_lock(self, timeout_sec=1800, poll=0.5):
        start = time.time()
        printed = False
        while True:
            try:
                fd = os.open(str(self.lock), os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                os.close(fd)
                print(f"[CACHE][LOCK][ACQUIRED] lock={self.lock}")
                return
            except FileExistsError:
                if not printed:
                    print(f"[CACHE][LOCK][WAIT] lock={self.lock} timeout={timeout_sec}s poll={poll}s")
                    printed = True
                if time.time() - start > timeout_sec:
                    raise TimeoutError("Timeout waiting for cache build lock")
                time.sleep(poll)

    def _release_lock(self):
        try:
            if self.lock.exists():
                self.lock.unlink()
        except Exception:
            pass

    def save(
        self,
        X_train, X_val, X_test,
        y_train, y_val, y_test,
        class_weights,
        csv_path: str,
        config: dict,
        extra: dict,
    ):
        # åŸå­æ€§å¯«å…¥ï¼šå…ˆå¯« tmp å† replace

        self._acquire_lock()
        try:
            # å†ç¢ºèªä¸€æ¬¡åˆ¥äººå·²ç¶“å»ºå¥½äº†
            ok, reason = self.is_valid(csv_path, config)
            if ok:
                print(f"[DataCache] another process built cache already ({reason})")
                return

            def _atomic_save(arr, path: Path):
                tmp = Path(str(path) + ".tmp")
                with open(tmp, "wb") as f:
                    np.save(f, arr, allow_pickle=False)
                    f.flush()
                    os.fsync(f.fileno())
                os.replace(tmp, path)


            # å¼·åˆ¶ dtype
            X_train = np.asarray(X_train, dtype=np.float32)
            X_val   = np.asarray(X_val, dtype=np.float32)
            X_test  = np.asarray(X_test, dtype=np.float32)

            # yï¼šäºŒå…ƒæ˜¯ float32 shape (N,1)ï¼Œå¤šé¡æ˜¯ int64 shape (N,)
            y_train = np.asarray(y_train)
            y_val   = np.asarray(y_val)
            y_test  = np.asarray(y_test)

            class_weights = np.asarray(class_weights, dtype=np.float32)

            _atomic_save(X_train, self.x_train)
            _atomic_save(X_val,   self.x_val)
            _atomic_save(X_test,  self.x_test)
            _atomic_save(y_train, self.y_train)
            _atomic_save(y_val,   self.y_val)
            _atomic_save(y_test,  self.y_test)
            _atomic_save(class_weights, self.cw)

            meta = self._make_meta(csv_path, config=config, extra=extra)
            self._write_meta_atomic(meta)

        finally:
            self._release_lock()

# ä¸»è³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå«å¿«å–ï¼‰
def load_data_with_cache(datapath: str):
    t0 = time.time()
    """Load tabular data with disk cache (DataCache) under CACHE_DIR (PVC)."""
    # print(f"Loading data from {datapath}")
    # çµ±ä¸€ log æ ¼å¼
    pod = os.environ.get("POD_NAME", "pod-unknown")
    pid = os.getpid()

    def _clog(event: str, **kv):
        # event: HIT/MISS/REBUILD/LOAD/SAVE/LOCK-WAIT...
        base = {
            "pod": pod,
            "pid": pid,
        }
        base.update(kv)
        extras = " ".join([f"{k}={v}" for k, v in base.items()])
        print(f"[CACHE][{event}] {extras}")

    _clog("ENTER", datapath=datapath)

    global scaling, encoding, drop_empty_rows
    scaling = "standard"
    encoding = "onehot"
    drop_empty_rows = True

    csv_path = os.path.join(datapath, "data.csv")

    #æ¯å€‹ Pod ç”¨è‡ªå·±çš„å­ç›®éŒ„ï¼Œé¿å…å…©å€‹ client pod åŒæ™‚å¯«å£åŒä¸€ä»½ cache
    cache_root = os.environ.get("CACHE_DIR", "/app/cache")
    # pod = os.environ.get("POD_NAME", "pod-unknown")
    cache_dir = str(Path(cache_root) / pod)

    cache = DataCache(cache_dir)

    # configï¼šåªè¦æ”¹äº†é€™äº›è¨­å®šï¼Œå°±æœƒè‡ªå‹•åˆ¤å®š cache invalid ,é‡å»º
    config = {
        "scaling": scaling,
        "encoding": encoding,
        "drop_empty_rows": drop_empty_rows,
        "split": {"test_size": 0.2, "val_from_train": 0.2, "random_state": 42, "stratify": True},
        "targets": ["dictionarybruteforce", "sqlinjection", "benigntraffic"],
        "label_col": None,  # ç¾åœ¨æ˜¯ auto_pick_label
    }

    ok, reason = cache.is_valid(csv_path, config)
    _clog("CHECK", cache_dir=cache_dir, csv=csv_path, ok=ok, reason=reason)
    if ok:
        _clog("HIT", cache_dir=cache_dir, reason=reason)
        X_train, X_val, X_test, y_train, y_val, y_test, cw_np, extra = cache.load()
        _clog("LOAD", sec=f"{time.time()-t0:.2f}", input_dim=extra.get("input_dim"), is_binary=extra.get("is_binary"))
        input_dim = int(extra["input_dim"])
        n_classes = int(extra["n_classes"])
        is_binary = bool(extra["is_binary"])

        class_weights = torch.tensor(cw_np, dtype=torch.float32)

        X_train_t = torch.from_numpy(X_train).float()
        X_val_t   = torch.from_numpy(X_val).float()
        X_test_t  = torch.from_numpy(X_test).float()

        if is_binary:
            # cache è£¡å·²ç¶“æ˜¯ (N,1) float32
            y_train_t = torch.from_numpy(y_train).float()
            y_val_t   = torch.from_numpy(y_val).float()
            y_test_t  = torch.from_numpy(y_test).float()
        else:
            y_train_t = torch.from_numpy(y_train).long()
            y_val_t   = torch.from_numpy(y_val).long()
            y_test_t  = torch.from_numpy(y_test).long()

        trainset = TensorDataset(X_train_t, y_train_t)
        valset   = TensorDataset(X_val_t, y_val_t)
        testset  = TensorDataset(X_test_t, y_test_t)

        _clog("RETURN", mode="HIT", train_n=len(trainset), val_n=len(valset), test_n=len(testset))
        return (
            DataLoader(trainset, batch_size=32, shuffle=True),
            DataLoader(valset, batch_size=32, shuffle=False),
            DataLoader(testset, batch_size=32, shuffle=False),
            input_dim,
            n_classes,
            is_binary,
            class_weights,
        )

    # print(f"[load_data_with_cache] cache MISS ({reason}) -> rebuild at {cache_dir}")
    _clog("MISS", cache_dir=cache_dir, reason=reason)
    t_build = time.time()

    # cache miss -> é‡æ–°å»ºç«‹
    (
        X_train,
        X_val,
        X_test,
        y_train_raw,
        y_val_raw,
        y_test_raw,
        label,
    ) = load_and_split(csv_path)

    (
        y_train_idx,
        y_val_idx,
        y_test_idx,
        num_classes,
        is_binary,
        le,
    ) = encode_labels(y_train_raw, y_val_raw, y_test_raw)

    pre, num_cols, cat_cols = fit_preprocessor(X_train)
    X_train_trans = transform_with_preprocessor(pre, X_train)
    X_val_trans   = transform_with_preprocessor(pre, X_val)
    X_test_trans  = transform_with_preprocessor(pre, X_test)

    input_dim = int(X_train_trans.shape[1])
    n_classes = 1 if is_binary else int(num_classes)

    # class weights
    cw_np = compute_class_weight("balanced", classes=np.unique(y_train_idx), y=y_train_idx).astype(np.float32)

    # y å­˜æˆ numpyï¼ˆbinary: float32 (N,1)ï¼Œmulti: int64 (N,)ï¼‰
    if is_binary:
        y_train_np = np.asarray(y_train_idx, dtype=np.float32).reshape(-1, 1)
        y_val_np   = np.asarray(y_val_idx, dtype=np.float32).reshape(-1, 1)
        y_test_np  = np.asarray(y_test_idx, dtype=np.float32).reshape(-1, 1)
    else:
        y_train_np = np.asarray(y_train_idx, dtype=np.int64)
        y_val_np   = np.asarray(y_val_idx, dtype=np.int64)
        y_test_np  = np.asarray(y_test_idx, dtype=np.int64)

    extra = {"input_dim": input_dim, "n_classes": n_classes, "is_binary": bool(is_binary)}
    _clog("REBUILD", phase="save_begin", cache_dir=cache_dir)
    cache.save(
        X_train_trans, X_val_trans, X_test_trans,
        y_train_np, y_val_np, y_test_np,
        cw_np,
        csv_path=csv_path,
        config=config,
        extra=extra,
    )
    _clog("REBUILD", phase="save_done", sec=f"{time.time()-t_build:.2f}", input_dim=input_dim, is_binary=is_binary)

    # å›å‚³ dataloaderï¼ˆç›´æ¥ç”¨å‰›ç®—å¥½çš„çµæœï¼Œä¸å†å¾æª”æ¡ˆè®€ä¸€æ¬¡ï¼‰
    class_weights = torch.tensor(cw_np, dtype=torch.float32)

    X_train_t = torch.tensor(X_train_trans, dtype=torch.float32)
    X_val_t   = torch.tensor(X_val_trans, dtype=torch.float32)
    X_test_t  = torch.tensor(X_test_trans, dtype=torch.float32)

    if is_binary:
        y_train_t = torch.tensor(y_train_np, dtype=torch.float32)
        y_val_t   = torch.tensor(y_val_np, dtype=torch.float32)
        y_test_t  = torch.tensor(y_test_np, dtype=torch.float32)
    else:
        y_train_t = torch.tensor(y_train_np, dtype=torch.long)
        y_val_t   = torch.tensor(y_val_np, dtype=torch.long)
        y_test_t  = torch.tensor(y_test_np, dtype=torch.long)

    trainset = TensorDataset(X_train_t, y_train_t)
    valset   = TensorDataset(X_val_t, y_val_t)
    testset  = TensorDataset(X_test_t, y_test_t)

    _clog("RETURN", mode="MISS->REBUILD", train_n=len(trainset), val_n=len(valset), test_n=len(testset))
    return (
        DataLoader(trainset, batch_size=32, shuffle=True),
        DataLoader(valset, batch_size=32, shuffle=False),
        DataLoader(testset, batch_size=32, shuffle=False),
        input_dim,
        n_classes,
        is_binary,
        class_weights,
    )


def client_fn(context: Context) -> fl.client.Client:
    """
    çµ¦ Flower SuperNode / ClientApp ä½¿ç”¨çš„å·¥å» å‡½æ•¸ã€‚
    ç›®æ¨™ï¼šé¿å…æ¯å€‹ round éƒ½é‡æ–°è®€ 369MB CSVã€é‡æ–°åˆ‡åˆ†ã€é‡æ–° fit preprocessorã€‚
    åŒä¸€å€‹ Pod å…§ï¼ˆåŒä¸€å€‹ Python processï¼‰åª load ä¸€æ¬¡ï¼Œå¾ŒçºŒ round ç”¨å¿«å–ã€‚
    """
    node_cfg = context.node_config or {}

    datapath = (
        os.environ.get("DATA_PATH")
        or node_cfg.get("datapath")
        or "./data"
    )
    print(f"[client_fn] node_id={context.node_id}, datapath={datapath}")

    #åªè¼‰å…¥ä¸€æ¬¡è³‡æ–™ï¼ˆå¿«å–ï¼‰
    global _DATA_CACHE
    if _DATA_CACHE is None:
        # print(f"[client_fn] cache miss > loading data from {datapath}")
        print(f"[client_fn] init -> loading (disk-cache may HIT) datapath={datapath}")

        #æ¸¬è©¦
        # _DATA_CACHE = load_data(datapath)
        _DATA_CACHE = load_data_with_cache(datapath)
        print("[client_fn] cache filled")
    else:
        print("[client_fn] cache hit -> reuse preloaded data")

    (
        trainloader,
        valloader,
        testloader,
        input_dim,
        n_classes,
        is_binary,
        class_weights,
    ) = _DATA_CACHE

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ¯æ¬¡ client_fn éƒ½é‡å»ºæ¨¡å‹æ˜¯å¯ä»¥ï¼›é‡é»æ˜¯è³‡æ–™ä¸è¦é‡è®€ï¼‰
    net = Net(input_dim=input_dim, hidden=[8, 8], n_classes=n_classes).to(DEVICE)

    class FlowerClient(fl.client.NumPyClient):
        def get_parameters(self, config):
            return [val.detach().cpu().numpy() for _, val in net.state_dict().items()]

        def set_parameters(self, parameters):
            params_dict = zip(net.state_dict().keys(), parameters)
            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
            net.load_state_dict(state_dict, strict=True)

        def fit(self, parameters, config):
            self.set_parameters(parameters)
            # åœˆæ•¸è¨­å®šepochs(è¼ª)
            local_epochs = int(config.get("local_epochs", 1))

            train(
                net,
                trainloader,
                valloader,
                epochs=local_epochs,
                class_weights=class_weights,
                is_binary=is_binary,
            )

            # ğŸ‘‰ ç”¨ validation set ç•¶ä½œ fit metrics
            val_loss, val_acc, val_f1 = test(net, valloader, is_binary)

            return (
                self.get_parameters({}),
                len(trainloader.dataset),
                {
                    "loss": float(val_loss),
                    "accuracy": float(val_acc),
                    "f1": float(val_f1),
                },
            )


        def evaluate(self, parameters, config):
            self.set_parameters(parameters)
            loss, accuracy, f1 = test(net, testloader, is_binary)
            return float(loss), len(testloader.dataset), {
                "accuracy": float(accuracy),
                "f1": float(f1),
            }

    return FlowerClient().to_client()


# é€™å€‹ app æœƒè¢« SuperNode æ‰¾åˆ°ä¸¦å•Ÿå‹•
app = ClientApp(client_fn=client_fn)
